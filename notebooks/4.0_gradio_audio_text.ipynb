{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING NLP WEB APPS WITH GRADIO, HUGGING FACE TRANSFORMERS, AND FB'S WAV2VEC2\n",
    "\n",
    "NLP is no longer limited to text inputs/outputs, and it is increasingly common to see models with image/audio/video-to-text capabilities, or vice versa. Multi-media web apps are far more complicated to build, but thankfully the latest version of Gradio supports these \"mixed-media\" web apps as well.\n",
    "\n",
    "The notebook will deal with a quick example for a speech-to-text app with audio inputs and text output. [Gradio's documentation](https://gradio.app/docs) suggests that the library supports video and image inputs just as easily.\n",
    "\n",
    "The key issue to note here is that both the Gradio and Transformers libraries are being updated at a fairly rapid clip, and certain models/features/classes may not be synced up to the latest versions. \n",
    "\n",
    "I've suppressed the warnings to keep the notebook clean, but if you comment out the warnings' filter, you'll notice the deprecation warnings for Wav2Vec2ForMaskedLM and Wav2Vec2Tokenizer. But their replacements don't work with Gradio. At some point, they'll sync up. For now, just keep that in mind in case the code breaks for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from transformers import Wav2Vec2ForMaskedLM, Wav2Vec2Tokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOAD WAV2VEC2 MODEL, DEFINE SPEECH-TO-TEXT FUNCTION\n",
    "\n",
    "There are about [400 versions of Wav2Vec2 models](https://huggingface.co/models?search=wav2vec2) on Hugging Face's model hub, so feel free to change one to suit your use case.\n",
    "\n",
    "The asr_transcript function should allow you to process long audio clips, just keep in mind that excessively long clips will take a long time to process and could well crash the app. I've tested the app on clips up to about 5-plus minutes.\n",
    "\n",
    "I've included 2 audio clips in this repo (in the data folder) - jfk.flac (62 seconds) and amanda_gorman.flac (5min 31s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForMaskedLM were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load wav2vec2 tokenizer and model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "\n",
    "model = Wav2Vec2ForMaskedLM.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "\n",
    "# define speech-to-text function\n",
    "def asr_transcript(audio_file):\n",
    "    transcript = \"\"\n",
    "\n",
    "    # Stream over 20 seconds chunks\n",
    "    stream = librosa.stream(\n",
    "        audio_file.name, block_length=20, frame_length=16000, hop_length=16000\n",
    "    )\n",
    "\n",
    "    for speech in stream:\n",
    "        if len(speech.shape) > 1:\n",
    "            speech = speech[:, 0] + speech[:, 1]\n",
    "\n",
    "        input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "        transcript += transcription.lower() + \" \"\n",
    "\n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DEFINE GRADIO INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradio_ui = gr.Interface(\n",
    "    fn=asr_transcript,\n",
    "    title=\"Speech-to-Text with HuggingFace+Wav2Vec2\",\n",
    "    description=\"Upload an audio clip, and let AI do the hard work of transcribing\",\n",
    "    inputs=gr.inputs.Audio(label=\"Upload Audio File\", type=\"file\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Auto-Transcript\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LAUNCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa20025e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gradio_ui.launch(share=True)\n",
    "gradio_ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
